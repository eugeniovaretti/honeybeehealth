---
title: "BNP clustering wrt to Varroa and Temp min and analyse colony lost with a functional approach"
output: html_notebook
---
```{r message=FALSE, warning=FALSE, echo=FALSE}
#if not installed install all the following libraries
library(tseries)
library(visdat)
library(raster)
library(ggplot2)
library(gridExtra)
library(forecast)
library(svglite)
library(roahd)
```
This notebook will (attempt to) cluster the timeseries associated to the US wrt to Varroa, Minimum Temperature and Precipitation in order to test if they are factors for colony_lost.
The clustering method will follow a Bayesian Non Parametric approach trhough the extension
of the Bayesmix Library (done for the Bayesian Project).
We first cluster by Varroa, choosing then the best cluster using a loss function (eg Binder / VI).
We then cluster by Temp Min (and Precipitation), following the same approach.
We will then put together the results of the two MCMC and apply again a loss function calculating
a Posterior Similarity Matrix (useful to compute the probability that 2 units are clustered together),
and extract the "best" clustering, i.e the cluster that minimizes the loss function.
Our main assumptions are (we will verify them later):
- the time series are AR(1);
- Earth is spherical (that's because we need to calculate the distances between coordinate lonlat -> we use Haversine Formula)

For the Varroa clustering: the value associated to the coordinate point i is relative to the centroid of the state.

Let's prepare our dataset:
#Let's study our coordinate:
```{r}
data <- read.csv("../../data/new_data/data_bystate_temp_perc.csv")
coord <- read.csv("../../data/state_coords_lon_lat.csv")

# remove data relative to hawaii & other state

data <- data[-c(which(data$state == "hawaii"),
               which(data$state == "other states")),]
coord <- coord[-c(which(coord$state == "hawaii"),
               which(coord$state == "other states")),]

```
Let's build the matrix n_state * time univariate (separately for Varroa and MinTemp)
```{r}
library(dplyr)
data_varr <- data[,c(1,2,3,11)]
ts_varroa <- data_varr %>% tidyr::pivot_wider(
  names_from = c("year", "months"), 
  values_from = Varroa.mites,
  values_fill = NULL
)

data_mintemp <- data[,c(1,2,3,18)]
ts_mintemp <- data_mintemp %>% tidyr::pivot_wider(
  names_from = c("year", "months"), 
  values_from = MinimumTemperature,
  values_fill = NULL
)

```

Now let's check if the rows of the two matrices coincide with the row of the coordinates:

```{r}
which(!(coord$state == ts_varroa$state))
which(!(coord$state == ts_mintemp$state))
```
Let's explore a bit coord data to see which parameter we should put for the clustering-boundary:
```{r}
cost <- pi/180
n <- nrow(coord)
  dist_matrix <- matrix(NA, n, n)
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      lat1 <- coord[i, 1]*cost
      lon1 <- coord[i, 2]*cost
      lat2 <- coord[j, 1]*cost
      lon2 <- coord[j, 2]*cost
      a <- sin((lat2 - lat1)/2)^2 + cos(lat1) * cos(lat2) * sin((lon2 - lon1)/2)^2
      c <- 2 * atan2(sqrt(a), sqrt(1 - a))
      d <- 6371 * c
      dist_matrix[i, j] <- d
      dist_matrix[j, i] <- d
    }
  }
#dist_matrix
  
  matd3 <- lower.tri(dist_matrix, diag=FALSE)*dist_matrix
  matd3 <- sort(as.numeric(matd3[!is.na(matd3) & matd3>0]))
list(
  range = range(matd3),
  mean_d = mean(matd3),
  quantiles = quantile(matd3, c(0.05,0.25,0.5,0.75, 0.8, 0.95))
)

```

Since we know very well how to cluster data in a Bayesian NP approach, let's try to model our data, doing a bit of model selection:
```{r}
# remove means from the time series
locations = unlist(ts_varroa[,1])
ts = t(ts_varroa[,-1])
p = length(locations)
medie = colMeans(ts)
ts_no_mean = ts
for (i in 1:p) {
  ts_no_mean[,i] = ts[,i] - medie[i]
}
```
Look at some random Acf:
```{r}
n_plot = 3
plots= vector("list",n_plot*n_plot)
random_indexes = sample(1:p, n_plot*n_plot)
j = 1
for (i in random_indexes) {
  plots[[j]] = ggAcf(ts(ts_no_mean[,i]), lag.max = 29) + labs(title = locations[i])
  j = j+1
}
plot_ACF = do.call(grid.arrange, c(plots, ncol = n_plot, nrow = n_plot))
#install.packages("svglite")
#ggsave("output_plot/ts_acf_random.svg", plot_ACF, width = 10, height = 10)
```
```{r}
# look at some random Pacf
n_plot = 3
plots= vector("list",n_plot*n_plot)
j = 1
for (i in random_indexes) {
  plots[[j]] = ggPacf(ts(ts_no_mean[,i]), lag.max = 29) + labs(title = locations[i])
  j = j+1
}
do.call(grid.arrange, c(plots, ncol = n_plot, nrow = n_plot))
```
It doesn't look like an AR(1)!! Let's try to remove seasonality!  
The problem is that we miss 2019Q2 so let's go fancy :).
1 - Use data till 2019Q1, remove seasonality and fit an AR1 on data.  
2 - Forecast the 2019Q2 using fitted coefficient.  
3 - Inverse the method in order to have the forecast of the "true" non-detrended value  
4 - Detrend everything and use the whole ts to cluster:  

```{r}
#1
ind_Q1_2019 <- which(rownames(ts_no_mean) == "2019_Q1")
reduced_ts_no_mean <- ts_no_mean[seq(1,ind_Q1_2019,by=1),]
#remove seasonality
reduced_ts_no_mean_no_seas <- diff(reduced_ts_no_mean, differences=1, lag=4)
```

Let's see if an AR(1) is good for our data wrt any other model of the family ARIMA
```{r}
fitted_models_AR1 = vector(mode="list",length = p)
fitted_models_best = vector(mode="list",length = p)
coefficienti_AR1 = rep(0,p)
for(i in 1:p)
{
  fitted_models_AR1[[i]] = arima(reduced_ts_no_mean_no_seas[,i], order = c(1,0,0), include.mean = FALSE)
  coefficienti_AR1[i] = fitted_models_AR1[[i]]$coef
  fitted_models_best[[i]] = auto.arima(reduced_ts_no_mean_no_seas[,i])
}

aic_AR1 = rep(0,p)
aic_best = rep(0,p)
for(i in 1:p)
{
  aic_AR1[i] = fitted_models_AR1[[i]]$aic
  aic_best[i] = fitted_models_best[[i]]$aic
}
plot_aic = plot(aic_AR1, pch=19, xlab = "Locations", ylab="aic",type='l', main="AIC comparison between AR(1) and the best ARIMA model")
points(aic_best, pch=19, col="red",type='l')
legend("topleft", legend = c("AR(1)", "Best ARIMA model"), fill = c("black","red"), bty="n", cex=0.8)
```
As we can see from the AIC the model is very good wrt the others (Occam's rasor).  
Let's forecast the value of Q2_2019 for each state. Our model now is:
$$
y_{i,t} = \alpha_i*y_{i,t-1} + \epsilon_i
$$
So we can use $\hat \alpha_i$:
```{r}
#2:
reduced_ts_no_mean_no_seas_2019Q2 <- coefficienti_AR1 *
                            reduced_ts_no_mean_no_seas["2019_Q1",]
aug_reduced_ts_no_mean_no_seas <- rbind(
  reduced_ts_no_mean_no_seas, "2019_Q2"=reduced_ts_no_mean_no_seas_2019Q2
)

```

Now it's time "come back" to the seasonality to merge the datasets:
```{r}
aug_reduced_ts_no_mean <- rbind(
  reduced_ts_no_mean,
  "2019_Q2" = reduced_ts_no_mean_no_seas_2019Q2 + reduced_ts_no_mean["2018_Q2",]
)


complete_aug_reduced_ts_no_mean <- rbind(
  aug_reduced_ts_no_mean,
  ts_no_mean[seq(which(rownames(ts_no_mean)=="2019_Q3"),
                 which(rownames(ts_no_mean)=="2022_Q2")),]
)

```
Let's look at some plots:
```{r}
matplot(complete_aug_reduced_ts_no_mean, type="l")
n_plot = 3
plots= vector("list",n_plot*n_plot)
random_indexes = sample(1:p, n_plot*n_plot)
j = 1
for (i in random_indexes) {
  plots[[j]] = ggAcf(ts(complete_aug_reduced_ts_no_mean[,i]), lag.max = 30) + labs(title = locations[i])
  j = j+1
}
plot_ACF = do.call(grid.arrange, c(plots, ncol = n_plot, nrow = n_plot))
```
Let's try to remove seasonality:
```{r}
complete_aug_reduced_ts_no_mean_no_seas <- diff(complete_aug_reduced_ts_no_mean, differences=1, lag=4)
n_plot = 3
plots= vector("list",n_plot*n_plot)
random_indexes = sample(1:p, n_plot*n_plot)
j = 1
for (i in random_indexes) {
  plots[[j]] = ggPacf(ts(complete_aug_reduced_ts_no_mean_no_seas[,i]), lag.max = 30) + labs(title = locations[i])
  j = j+1
}
plot_ACF = do.call(grid.arrange, c(plots, ncol = n_plot, nrow = n_plot))

```
Let's see if an ARMA(1) would perform well:
```{r}
fitted_models_AR1 = vector(mode="list",length = p)
fitted_models_best = vector(mode="list",length = p)
coefficienti_AR1 = rep(0,p)
for(i in 1:p)
{
  fitted_models_AR1[[i]] = arima(complete_aug_reduced_ts_no_mean_no_seas[,i], order = c(1,0,0), include.mean = FALSE)
  coefficienti_AR1[i] = fitted_models_AR1[[i]]$coef
  fitted_models_best[[i]] = auto.arima(complete_aug_reduced_ts_no_mean_no_seas[,i])
}

aic_AR1 = rep(0,p)
aic_best = rep(0,p)
for(i in 1:p)
{
  aic_AR1[i] = fitted_models_AR1[[i]]$aic
  aic_best[i] = fitted_models_best[[i]]$aic
}
plot_aic = plot(aic_AR1, pch=19, xlab = "Locations", ylab="aic",type='l', main="AIC comparison between AR(1) and the best ARIMA model (no seasonality)")
points(aic_best, pch=19, col="red",type='l')
legend("topleft", legend = c("AR(1)", "Best ARIMA model"), fill = c("black","red"), bty="n", cex=0.8)
```

Let's compare the result with the original dataset (not augmented -> no 2019_Q2) without removing seas:
```{r}
fitted_models_AR1 = vector(mode="list",length = p)
fitted_models_best = vector(mode="list",length = p)
coefficienti_AR1 = rep(0,p)
for(i in 1:p)
{
  fitted_models_AR1[[i]] = arima(ts_no_mean[,i], order = c(1,0,0), include.mean = FALSE)
  coefficienti_AR1[i] = fitted_models_AR1[[i]]$coef
  fitted_models_best[[i]] = auto.arima(ts_no_mean[,i])
}

aic_AR1 = rep(0,p)
aic_best = rep(0,p)
for(i in 1:p)
{
  aic_AR1[i] = fitted_models_AR1[[i]]$aic
  aic_best[i] = fitted_models_best[[i]]$aic
}
plot_aic = plot(aic_AR1, pch=19, xlab = "Locations", ylab="aic",type='l', main="AIC comparison between AR(1) and the best ARIMA model")
points(aic_best, pch=19, col="red",type='l')
legend("topleft", legend = c("AR(1)", "Best ARIMA model"), fill = c("black","red"), bty="n", cex=0.8)
```
As we can see, compaaring the two plots the detrended data are better to be modeled as AR(1), so let's stick with them and cluster wrt them!
Let's do a frequentist estimate of the parameter we want to pass to the algorithm.
```{r}
  p = ncol(complete_aug_reduced_ts_no_mean_no_seas)
  fitted_models_AR1 = vector(mode="list",length = p)
  coefficienti_AR = rep(0,p)
  sigma_2_i = rep(0,p)
  
  for(i in 1:p)
  {
    fitted_models_AR1[[i]] = arima(complete_aug_reduced_ts_no_mean_no_seas[,i], order = c(1,0,0), include.mean = FALSE)
    coefficienti_AR[i] = fitted_models_AR1[[i]]$coef
    sigma_2_i[i] = fitted_models_AR1[[i]]$sigma2
  }
  
  # stimo i parametri delle prior
  ### rho_h ~ N(rho_0, sigma_2_h / lambda )
  rho_0 = mean(coefficienti_AR)
  sigma_2_avg = mean(sigma_2_i)
  var_rho_h = var(coefficienti_AR)
  lambda = (sigma_2_avg/var_rho_h)
  alpha = (sigma_2_avg)^2 / var(sigma_2_i) + 2
  beta = (alpha-1)*sigma_2_avg
  
  parametri = list(rho_0 = rho_0,
                   lambda = lambda,
                   alpha = alpha,
                   beta = beta)
```
Let's save the output and datasets
```{r}
folder="data_output/"
write.table(t(complete_aug_reduced_ts_no_mean_no_seas),sep=",", row.names = FALSE, col.names=FALSE , file = paste(folder, "ts.csv", sep=""))
write.table(t(ts),sep=",", row.names = FALSE, col.names=FALSE , file = paste(folder, "ts_mean.csv", sep=""))
write.table(coord[,-3],sep=",", row.names = FALSE, col.names=FALSE , file = paste(folder, "coord.csv", sep=""))
```
From the literature we can see that the approximate radius of the various climatic zones in USA is  1900 km (this is the approximate distance between Oregon and North Dakota). But let's analyse the distance between our data first:
```{r}

```

Now, we can apply the library bayesmix to cluster our data.

Import the output of bayesmix:
```{r}
source("utils_e/plot_clustering.R")

num_clust = read.csv("data_output/varroa/numclust.csv", header = FALSE)
names(num_clust)="n"

partitions = read.csv("data_output/varroa/clustering.csv", header = FALSE)
uvc <- read.csv("data_output/varroa/stateparams.csv", header = FALSE)

M <- 0.7286
a <- 1900
label <- F
(ob6 = plot_clustering(partitions, coord[,-3], "VI" ,M, a, ts_varroa[,-1], label = label, unique_vals_chain = uvc ))

```
```{r}
ob6$matplot
```


```{r}
ob6$posterior_unique_vals
ob6$best_clus
```


```{r}
# questo lo faccio quando ho poi tutto bello pronto il prog di bayesian così ci metto di -
# assumo di avere vettore cluster allocation (simulo 4 cluster) per i 46 stati
set.seed(123)
cluster_alloc <- ob6$best_clus

```


Now, we can use a functional approach on loss data in order to see if there are differences for the medians of the groups.
Let's prepare tha dataset transforming it functionally

```{r}
length_ts <- dim(ts_varroa)[2]-1
grid <- seq(1,length_ts, by=1)

fd_varroa <- fData(grid,ts_varroa[,-1])
fd_tempmin <- fData(grid, ts_varroa[,-1])
```

JUST to *explore* the result obtained from let's plot them coloring wrt the cluster 
(it's wanted to see that they are separated in some sense)
```{r}
plot(fd_varroa, col="lightgrey", main="Varroa trend in the different clusters", lwd=0.1)
clus1 <- which(cluster_alloc==1)
clus2 <- which(cluster_alloc==2)
clus3 <- which(cluster_alloc==3)


median1 <- median_fData(fd_varroa[clus1,], type="MBD")
median2 <- median_fData(fd_varroa[clus2,], type="MBD")
median3 <- median_fData(fd_varroa[clus3,], type="MBD")

lines(grid, median1$value, col=1, lwd = 2)
lines(grid, median2$value, col=2, lwd = 2)
lines(grid, median3$value, col=3, lwd = 2)

```

```{r}
plot(fd_varroa[clus1,], col="lightgrey", main="Varroa trend in the different clusters", lwd=0.1)
lines(grid, median1$value, col=1, lwd = 2)
plot(fd_varroa[clus2,], col="lightgrey", main="Varroa trend in the different clusters", lwd=0.1)
lines(grid, median2$value, col=2, lwd = 2)
```
Lets see if in the two clusters there are outliers:

```{r}
bxplot = invisible(fbplot(fd_varroa[clus1,], main = "Magnitude Outliers", xlab = "Quarter", ylab = "Colony%"))
bxplot = invisible(fbplot(fd_varroa[clus2,], main = "Magnitude Outliers", xlab = "Quarter", ylab = "Colony%"))
```


Let's prepare the dataset for the colony losses (USO PCT ORA):
```{r}
data_colonylost <- data[,c(1,2,3,7)]
ts_colonylost <- data_colonylost %>% tidyr::pivot_wider(
  names_from = c("year", "months"), 
  values_from = colony_lost_pct, ####se vui % cambia qua
  values_fill = NULL
)

fd_colonylost <- fData(grid,ts_colonylost[,-1])
plot(fd_colonylost, col=as.numeric(cluster_alloc), main="Colony lost (pct) trend in the different clusters", xlab = "Quarter", ylab = "Colony%")
```
Let's compute the median of the clusters (using modified band depth) and superimpose them on the plot:
```{r}
clus1 <- which(cluster_alloc==1)
clus2 <- which(cluster_alloc==2)
clus3 <- which(cluster_alloc==3)
clus4 <- which(cluster_alloc==4)
clus5 <- which(cluster_alloc==5)
clus6 <- which(cluster_alloc==6)



median1 <- median_fData(fd_colonylost[clus1,], type="MBD")
median2 <- median_fData(fd_colonylost[clus2,], type="MBD")
median3 <- median_fData(fd_colonylost[clus3,], type="MBD")
median4 <- median_fData(fd_colonylost[clus4,], type="MBD")
median5 <- median_fData(fd_colonylost[clus5,], type="MBD")
median6 <- median_fData(fd_colonylost[clus6,], type="MBD")


plot(fd_colonylost, col="darkgrey", main="Colony lost (pct) trend in the different clusters", xlab = "Quarter", ylab = "Colony%")
lines(grid, median1$value, col=1, lwd = 2)
lines(grid, median2$value, col=2, lwd = 2)
lines(grid, median3$value, col=3, lwd = 2)
lines(grid, median4$value, col=4, lwd = 2)
lines(grid, median5$value, col=5, lwd = 2)
lines(grid, median6$value, col=6, lwd = 2)


```
## Spot if we have outliers inside the group: #farlo in base a quanti gruppi troviamo
```{r}
bxplot = invisible(fbplot(fd_colonylost[clus1,], main = "Magnitude Outliers", xlab = "Quarter", ylab = "Colony%"))
bxplot = invisible(fbplot(fd_colonylost[clus2,], main = "Magnitude Outliers", xlab = "Quarter", ylab = "Colony%"))
bxplot = invisible(fbplot(fd_colonylost[clus3,], main = "Magnitude Outliers", xlab = "Quarter", ylab = "Colony%"))
bxplot = invisible(fbplot(fd_colonylost[clus4,], main = "Magnitude Outliers", xlab = "Quarter", ylab = "Colony%"))
```


```{r}
out_funct <- outliergram(fd_colonylost[clus1,])
out_funct$ID_outliers
```
If it is necessary we can remove outliers...

# Test difference between clusters
Now that we have a cluster wrt to Varroa, we want to test if the distribution of the colony_lost differ between the cluster. We test it using the functional medians, i.e:
H0 : med(c_1)=med(c_2)=...=med(c_i) vs H1: H0^c

Let's do a pairwise test first:  
## Clus 1-2
```{r}
x1_fd <- fd_colonylost[clus1,] #
x2_fd <- fd_colonylost[clus2,] #
medianx1 <- median1 #
medianx2 <- median2 #

datapooled=append_fData(x1_fd,x2_fd)

n=datapooled$N
n1=x1_fd$N
n2=x2_fd$N

meandiff = medianx1$values - medianx2$values
#plot(grid,meandiff,type = 'l')
T0=(sum(abs(meandiff)))
#And, Knowing that under H0 the two groups of curves are IID,
# my likelihood-invariant permutation scheme is of course label permutation, so:
B <- 1000
set.seed(2023)
T0_perm=numeric(B)

pb = progress::progress_bar$new(total = B,
                                format = " Processing [:bar] :percent eta: :eta")
for(b in 1:B){
  permutazione <- sample(n)
  datapooled_perm=datapooled[permutazione,]
  perm_1 = datapooled_perm[1:n1,] 
  perm_2 = datapooled_perm[(n1+1):n,]
  #se funzionale
  meandiff_p =median_fData(perm_1,type='MBD')$values-median_fData(perm_2,type='MBD')$values
  T0_perm[b]=(sum(abs(meandiff_p))) #set
  pb$tick()
}
```
Let's see the results:  
```{r}
pval = sum(T0_perm >= T0)/B
hist(T0_perm, main = "cluster1 vs cluster 2") #
abline(v=T0,col='green')
print(paste("pvalue for test cluster 1 vs cluster 2 is :", pval)) #
```

## Clus 1-3
```{r}
x1_fd <- fd_colonylost[clus1,]
x2_fd <- fd_colonylost[clus3,]
medianx1 <- median1
medianx2 <- median3

datapooled=append_fData(x1_fd,x2_fd)

n=datapooled$N
n1=x1_fd$N
n2=x2_fd$N

meandiff = medianx1$values - medianx2$values
#plot(grid,meandiff,type = 'l')
T0=(sum(abs(meandiff)))
#And, Knowing that under H0 the two groups of curves are IID,
# my likelihood-invariant permutation scheme is of course label permutation, so:
B <- 1000
set.seed(2023)
T0_perm=numeric(B)

pb = progress::progress_bar$new(total = B,
                                format = " Processing [:bar] :percent eta: :eta")
for(b in 1:B){
  permutazione <- sample(n)
  datapooled_perm=datapooled[permutazione,]
  perm_1 = datapooled_perm[1:n1,] 
  perm_2 = datapooled_perm[(n1+1):n,]
  #se funzionale
  meandiff_p =median_fData(perm_1,type='MBD')$values-median_fData(perm_2,type='MBD')$values
  T0_perm[b]=(sum(abs(meandiff_p))) #set
  pb$tick()
}
```
Let's see the results:  
```{r}
pval = sum(T0_perm >= T0)/B
hist(T0_perm, main = "cluster1 vs cluster 3")
abline(v=T0,col='green')
print(paste("pvalue for test cluster 1 vs cluster 3 is :", pval)) #
```

## Cluster 1-4
```{r}
x1_fd <- fd_colonylost[clus1,]
x2_fd <- fd_colonylost[clus4,]
medianx1 <- median1
medianx2 <- median4

datapooled=append_fData(x1_fd,x2_fd)

n=datapooled$N
n1=x1_fd$N
n2=x2_fd$N

meandiff = medianx1$values - medianx2$values
#plot(grid,meandiff,type = 'l')
T0=(sum(abs(meandiff)))
#And, Knowing that under H0 the two groups of curves are IID,
# my likelihood-invariant permutation scheme is of course label permutation, so:
B <- 1000
set.seed(2023)
T0_perm=numeric(B)

pb = progress::progress_bar$new(total = B,
                                format = " Processing [:bar] :percent eta: :eta")
for(b in 1:B){
  permutazione <- sample(n)
  datapooled_perm=datapooled[permutazione,]
  perm_1 = datapooled_perm[1:n1,] 
  perm_2 = datapooled_perm[(n1+1):n,]
  #se funzionale
  meandiff_p =median_fData(perm_1,type='MBD')$values-median_fData(perm_2,type='MBD')$values
  T0_perm[b]=(sum(abs(meandiff_p))) #set
  pb$tick()
}
```
Let's see the results:  
```{r}
pval = sum(T0_perm >= T0)/B
hist(T0_perm, main = "cluster1 vs cluster 3")
abline(v=T0,col='green')
print(paste("pvalue for test cluster 1 vs cluster 4 is :", pval)) #
```
#Cluseter 1-5
```{r}
x1_fd <- fd_colonylost[clus1,]
x2_fd <- fd_colonylost[clus5,]
medianx1 <- median1
medianx2 <- median5

datapooled=append_fData(x1_fd,x2_fd)

n=datapooled$N
n1=x1_fd$N
n2=x2_fd$N

meandiff = medianx1$values - medianx2$values
#plot(grid,meandiff,type = 'l')
T0=(sum(abs(meandiff)))
#And, Knowing that under H0 the two groups of curves are IID,
# my likelihood-invariant permutation scheme is of course label permutation, so:
B <- 1000
set.seed(2023)
T0_perm=numeric(B)

pb = progress::progress_bar$new(total = B,
                                format = " Processing [:bar] :percent eta: :eta")
for(b in 1:B){
  permutazione <- sample(n)
  datapooled_perm=datapooled[permutazione,]
  perm_1 = datapooled_perm[1:n1,] 
  perm_2 = datapooled_perm[(n1+1):n,]
  #se funzionale
  meandiff_p =median_fData(perm_1,type='MBD')$values-median_fData(perm_2,type='MBD')$values
  T0_perm[b]=(sum(abs(meandiff_p))) #set
  pb$tick()
}
```
Let's see the results:  
```{r}
pval = sum(T0_perm >= T0)/B
hist(T0_perm, main = "cluster1 vs cluster 3")
abline(v=T0,col='green')
print(paste("pvalue for test cluster 1 vs cluster 5 is :", pval)) #
```
# Clus 1-6
```{r}
x1_fd <- fd_colonylost[clus1,]
x2_fd <- fd_colonylost[clus6,]
medianx1 <- median1
medianx2 <- median6

datapooled=append_fData(x1_fd,x2_fd)

n=datapooled$N
n1=x1_fd$N
n2=x2_fd$N

meandiff = medianx1$values - medianx2$values
#plot(grid,meandiff,type = 'l')
T0=(sum(abs(meandiff)))
#And, Knowing that under H0 the two groups of curves are IID,
# my likelihood-invariant permutation scheme is of course label permutation, so:
B <- 1000
set.seed(2023)
T0_perm=numeric(B)

pb = progress::progress_bar$new(total = B,
                                format = " Processing [:bar] :percent eta: :eta")
for(b in 1:B){
  permutazione <- sample(n)
  datapooled_perm=datapooled[permutazione,]
  perm_1 = datapooled_perm[1:n1,] 
  perm_2 = datapooled_perm[(n1+1):n,]
  #se funzionale
  meandiff_p =median_fData(perm_1,type='MBD')$values-median_fData(perm_2,type='MBD')$values
  T0_perm[b]=(sum(abs(meandiff_p))) #set
  pb$tick()
}
```
Let's see the results:  
```{r}
pval = sum(T0_perm >= T0)/B
hist(T0_perm, main = "cluster1 vs cluster 3")
abline(v=T0,col='green')
print(paste("pvalue for test cluster 1 vs cluster 6 is :", pval)) #
```

#Cluster 2-3
```{r}
x1_fd <- fd_colonylost[clus2,]
x2_fd <- fd_colonylost[clus3,]
medianx1 <- median2
medianx2 <- median3

datapooled=append_fData(x1_fd,x2_fd)

n=datapooled$N
n1=x1_fd$N
n2=x2_fd$N

meandiff = medianx1$values - medianx2$values
#plot(grid,meandiff,type = 'l')
T0=(sum(abs(meandiff)))
#And, Knowing that under H0 the two groups of curves are IID,
# my likelihood-invariant permutation scheme is of course label permutation, so:
B <- 1000
set.seed(2023)
T0_perm=numeric(B)

pb = progress::progress_bar$new(total = B,
                                format = " Processing [:bar] :percent eta: :eta")
for(b in 1:B){
  permutazione <- sample(n)
  datapooled_perm=datapooled[permutazione,]
  perm_1 = datapooled_perm[1:n1,] 
  perm_2 = datapooled_perm[(n1+1):n,]
  #se funzionale
  meandiff_p =median_fData(perm_1,type='MBD')$values-median_fData(perm_2,type='MBD')$values
  T0_perm[b]=(sum(abs(meandiff_p))) #set
  pb$tick()
}
```
Let's see the results:  
```{r}
pval = sum(T0_perm >= T0)/B
hist(T0_perm, main = "cluster1 vs cluster 3")
abline(v=T0,col='green')
print(paste("pvalue for test cluster 2 vs cluster 3 is :", pval)) #
```

#Cluster 2-4
```{r}
x1_fd <- fd_colonylost[clus2,]
x2_fd <- fd_colonylost[clus4,]
medianx1 <- median2
medianx2 <- median4

datapooled=append_fData(x1_fd,x2_fd)

n=datapooled$N
n1=x1_fd$N
n2=x2_fd$N

meandiff = medianx1$values - medianx2$values
#plot(grid,meandiff,type = 'l')
T0=(sum(abs(meandiff)))
#And, Knowing that under H0 the two groups of curves are IID,
# my likelihood-invariant permutation scheme is of course label permutation, so:
B <- 1000
set.seed(2023)
T0_perm=numeric(B)

pb = progress::progress_bar$new(total = B,
                                format = " Processing [:bar] :percent eta: :eta")
for(b in 1:B){
  permutazione <- sample(n)
  datapooled_perm=datapooled[permutazione,]
  perm_1 = datapooled_perm[1:n1,] 
  perm_2 = datapooled_perm[(n1+1):n,]
  #se funzionale
  meandiff_p =median_fData(perm_1,type='MBD')$values-median_fData(perm_2,type='MBD')$values
  T0_perm[b]=(sum(abs(meandiff_p))) #set
  pb$tick()
}
```
Let's see the results:  
```{r}
pval = sum(T0_perm >= T0)/B
hist(T0_perm, main = "cluster1 vs cluster 4")
abline(v=T0,col='green')
print(paste("pvalue for test cluster 2 vs cluster 4 is :", pval)) #
```
#Cluster 2-5
```{r}
x1_fd <- fd_colonylost[clus2,]
x2_fd <- fd_colonylost[clus5,]
medianx1 <- median2
medianx2 <- median5

datapooled=append_fData(x1_fd,x2_fd)

n=datapooled$N
n1=x1_fd$N
n2=x2_fd$N

meandiff = medianx1$values - medianx2$values
#plot(grid,meandiff,type = 'l')
T0=(sum(abs(meandiff)))
#And, Knowing that under H0 the two groups of curves are IID,
# my likelihood-invariant permutation scheme is of course label permutation, so:
B <- 1000
set.seed(2023)
T0_perm=numeric(B)

pb = progress::progress_bar$new(total = B,
                                format = " Processing [:bar] :percent eta: :eta")
for(b in 1:B){
  permutazione <- sample(n)
  datapooled_perm=datapooled[permutazione,]
  perm_1 = datapooled_perm[1:n1,] 
  perm_2 = datapooled_perm[(n1+1):n,]
  #se funzionale
  meandiff_p =median_fData(perm_1,type='MBD')$values-median_fData(perm_2,type='MBD')$values
  T0_perm[b]=(sum(abs(meandiff_p))) #set
  pb$tick()
}
```
Let's see the results:  
```{r}
pval = sum(T0_perm >= T0)/B
hist(T0_perm, main = "cluster1 vs cluster 3")
abline(v=T0,col='green')
print(paste("pvalue for test cluster 2 vs cluster 5 is :", pval)) #
```
#Cluster 2-6
```{r}
x1_fd <- fd_colonylost[clus2,]
x2_fd <- fd_colonylost[clus6,]
medianx1 <- median2
medianx2 <- median6

datapooled=append_fData(x1_fd,x2_fd)

n=datapooled$N
n1=x1_fd$N
n2=x2_fd$N

meandiff = medianx1$values - medianx2$values
#plot(grid,meandiff,type = 'l')
T0=(sum(abs(meandiff)))
#And, Knowing that under H0 the two groups of curves are IID,
# my likelihood-invariant permutation scheme is of course label permutation, so:
B <- 1000
set.seed(2023)
T0_perm=numeric(B)

pb = progress::progress_bar$new(total = B,
                                format = " Processing [:bar] :percent eta: :eta")
for(b in 1:B){
  permutazione <- sample(n)
  datapooled_perm=datapooled[permutazione,]
  perm_1 = datapooled_perm[1:n1,] 
  perm_2 = datapooled_perm[(n1+1):n,]
  #se funzionale
  meandiff_p =median_fData(perm_1,type='MBD')$values-median_fData(perm_2,type='MBD')$values
  T0_perm[b]=(sum(abs(meandiff_p))) #set
  pb$tick()
}
```
Let's see the results:  
```{r}
pval = sum(T0_perm >= T0)/B
hist(T0_perm, main = "cluster1 vs cluster 3")
abline(v=T0,col='green')
print(paste("pvalue for test cluster 2 vs cluster 6 is :", pval)) #
```
#Cluster 3-4
```{r}
x1_fd <- fd_colonylost[clus3,]
x2_fd <- fd_colonylost[clus4,]
medianx1 <- median3
medianx2 <- median4

datapooled=append_fData(x1_fd,x2_fd)

n=datapooled$N
n1=x1_fd$N
n2=x2_fd$N

meandiff = medianx1$values - medianx2$values
#plot(grid,meandiff,type = 'l')
T0=(sum(abs(meandiff)))
#And, Knowing that under H0 the two groups of curves are IID,
# my likelihood-invariant permutation scheme is of course label permutation, so:
B <- 1000
set.seed(2023)
T0_perm=numeric(B)

pb = progress::progress_bar$new(total = B,
                                format = " Processing [:bar] :percent eta: :eta")
for(b in 1:B){
  permutazione <- sample(n)
  datapooled_perm=datapooled[permutazione,]
  perm_1 = datapooled_perm[1:n1,] 
  perm_2 = datapooled_perm[(n1+1):n,]
  #se funzionale
  meandiff_p =median_fData(perm_1,type='MBD')$values-median_fData(perm_2,type='MBD')$values
  T0_perm[b]=(sum(abs(meandiff_p))) #set
  pb$tick()
}
```
Let's see the results:  
```{r}
pval = sum(T0_perm >= T0)/B
hist(T0_perm, main = "cluster3 vs cluster 4")
abline(v=T0,col='green')
print(paste("pvalue for test cluster 2 vs cluster 3 is :", pval)) #
```
#Cluster 3-5
```{r}
x1_fd <- fd_colonylost[clus3,]
x2_fd <- fd_colonylost[clus5,]
medianx1 <- median3
medianx2 <- median5

datapooled=append_fData(x1_fd,x2_fd)

n=datapooled$N
n1=x1_fd$N
n2=x2_fd$N

meandiff = medianx1$values - medianx2$values
#plot(grid,meandiff,type = 'l')
T0=(sum(abs(meandiff)))
#And, Knowing that under H0 the two groups of curves are IID,
# my likelihood-invariant permutation scheme is of course label permutation, so:
B <- 1000
set.seed(2023)
T0_perm=numeric(B)

pb = progress::progress_bar$new(total = B,
                                format = " Processing [:bar] :percent eta: :eta")
for(b in 1:B){
  permutazione <- sample(n)
  datapooled_perm=datapooled[permutazione,]
  perm_1 = datapooled_perm[1:n1,] 
  perm_2 = datapooled_perm[(n1+1):n,]
  #se funzionale
  meandiff_p =median_fData(perm_1,type='MBD')$values-median_fData(perm_2,type='MBD')$values
  T0_perm[b]=(sum(abs(meandiff_p))) #set
  pb$tick()
}
```
Let's see the results:  
```{r}
pval = sum(T0_perm >= T0)/B
hist(T0_perm, main = "cluster1 vs cluster 3")
abline(v=T0,col='green')
print(paste("pvalue for test cluster 3 vs cluster 5 is :", pval)) #
```
#Cluster 3-6
```{r}
x1_fd <- fd_colonylost[clus3,]
x2_fd <- fd_colonylost[clus6,]
medianx1 <- median3
medianx2 <- median6

datapooled=append_fData(x1_fd,x2_fd)

n=datapooled$N
n1=x1_fd$N
n2=x2_fd$N

meandiff = medianx1$values - medianx2$values
#plot(grid,meandiff,type = 'l')
T0=(sum(abs(meandiff)))
#And, Knowing that under H0 the two groups of curves are IID,
# my likelihood-invariant permutation scheme is of course label permutation, so:
B <- 1000
set.seed(2023)
T0_perm=numeric(B)

pb = progress::progress_bar$new(total = B,
                                format = " Processing [:bar] :percent eta: :eta")
for(b in 1:B){
  permutazione <- sample(n)
  datapooled_perm=datapooled[permutazione,]
  perm_1 = datapooled_perm[1:n1,] 
  perm_2 = datapooled_perm[(n1+1):n,]
  #se funzionale
  meandiff_p =median_fData(perm_1,type='MBD')$values-median_fData(perm_2,type='MBD')$values
  T0_perm[b]=(sum(abs(meandiff_p))) #set
  pb$tick()
}
```
Let's see the results:  
```{r}
pval = sum(T0_perm >= T0)/B
hist(T0_perm, main = "cluster1 vs cluster 3")
abline(v=T0,col='green')
print(paste("pvalue for test cluster 2 vs cluster 3 is :", pval)) #
```
#Cluster 4-5
```{r}
x1_fd <- fd_colonylost[clus4,]
x2_fd <- fd_colonylost[clus5,]
medianx1 <- median4
medianx2 <- median5

datapooled=append_fData(x1_fd,x2_fd)

n=datapooled$N
n1=x1_fd$N
n2=x2_fd$N

meandiff = medianx1$values - medianx2$values
#plot(grid,meandiff,type = 'l')
T0=(sum(abs(meandiff)))
#And, Knowing that under H0 the two groups of curves are IID,
# my likelihood-invariant permutation scheme is of course label permutation, so:
B <- 1000
set.seed(2023)
T0_perm=numeric(B)

pb = progress::progress_bar$new(total = B,
                                format = " Processing [:bar] :percent eta: :eta")
for(b in 1:B){
  permutazione <- sample(n)
  datapooled_perm=datapooled[permutazione,]
  perm_1 = datapooled_perm[1:n1,] 
  perm_2 = datapooled_perm[(n1+1):n,]
  #se funzionale
  meandiff_p =median_fData(perm_1,type='MBD')$values-median_fData(perm_2,type='MBD')$values
  T0_perm[b]=(sum(abs(meandiff_p))) #set
  pb$tick()
}
```
Let's see the results:  
```{r}
pval = sum(T0_perm >= T0)/B
hist(T0_perm, main = "cluster1 vs cluster 3")
abline(v=T0,col='green')
print(paste("pvalue for test cluster 4 vs cluster 5 is :", pval)) #
```
#Cluster 4-6
```{r}
x1_fd <- fd_colonylost[clus4,]
x2_fd <- fd_colonylost[clus6,]
medianx1 <- median4
medianx2 <- median6

datapooled=append_fData(x1_fd,x2_fd)

n=datapooled$N
n1=x1_fd$N
n2=x2_fd$N

meandiff = medianx1$values - medianx2$values
#plot(grid,meandiff,type = 'l')
T0=(sum(abs(meandiff)))
#And, Knowing that under H0 the two groups of curves are IID,
# my likelihood-invariant permutation scheme is of course label permutation, so:
B <- 1000
set.seed(2023)
T0_perm=numeric(B)

pb = progress::progress_bar$new(total = B,
                                format = " Processing [:bar] :percent eta: :eta")
for(b in 1:B){
  permutazione <- sample(n)
  datapooled_perm=datapooled[permutazione,]
  perm_1 = datapooled_perm[1:n1,] 
  perm_2 = datapooled_perm[(n1+1):n,]
  #se funzionale
  meandiff_p =median_fData(perm_1,type='MBD')$values-median_fData(perm_2,type='MBD')$values
  T0_perm[b]=(sum(abs(meandiff_p))) #set
  pb$tick()
}
```
Let's see the results:  
```{r}
pval = sum(T0_perm >= T0)/B
hist(T0_perm, main = "cluster1 vs cluster 3")
abline(v=T0,col='green')
print(paste("pvalue for test cluster 4 vs cluster 6 is :", pval)) #
```
#Cluster 5-6
```{r}
x1_fd <- fd_colonylost[clus5,]
x2_fd <- fd_colonylost[clus6,]
medianx1 <- median4
medianx2 <- median6

datapooled=append_fData(x1_fd,x2_fd)

n=datapooled$N
n1=x1_fd$N
n2=x2_fd$N

meandiff = medianx1$values - medianx2$values
#plot(grid,meandiff,type = 'l')
T0=(sum(abs(meandiff)))
#And, Knowing that under H0 the two groups of curves are IID,
# my likelihood-invariant permutation scheme is of course label permutation, so:
B <- 1000
set.seed(2023)
T0_perm=numeric(B)

pb = progress::progress_bar$new(total = B,
                                format = " Processing [:bar] :percent eta: :eta")
for(b in 1:B){
  permutazione <- sample(n)
  datapooled_perm=datapooled[permutazione,]
  perm_1 = datapooled_perm[1:n1,] 
  perm_2 = datapooled_perm[(n1+1):n,]
  #se funzionale
  meandiff_p =median_fData(perm_1,type='MBD')$values-median_fData(perm_2,type='MBD')$values
  T0_perm[b]=(sum(abs(meandiff_p))) #set
  pb$tick()
}
```
Let's see the results:  
```{r}
pval = sum(T0_perm >= T0)/B
hist(T0_perm, main = "cluster1 vs cluster 3")
abline(v=T0,col='green')
print(paste("pvalue for test cluster 5 vs cluster 6 is :", pval)) #
```











# Cluster by Minimum temperature 
Let's carry out the same analysis, but clustering wrt min temp:
Since we know very well how to cluster data in a Bayesian NP approach, let's try to model our data, doing a bit of model selection.
Let's import firstly all the raw dataset:
```{r}
ambient <- read.csv("../../data/new_data/temp_prec_trimesters.csv")
mq <- ambient$months
mq[mq=="January-March"] <- "Q1"
mq[mq=="April-June"] <- "Q2"
mq[mq=="July-September"] <- "Q3"
mq[mq=="October-December"] <- "Q4"
ambient$mq <- mq



data_mintemp <- ambient[,c(1,9,4,8)]


ts_mintemp <- data_mintemp %>% tidyr::pivot_wider(
  names_from = c("year", "mq"), 
  values_from = MinimumTemperature,
  values_fill = NULL
)

ts_mintemp$state <- tolower(ts_mintemp$state)

#check which location is in ts_ that is not in location
#res <- ts_mintemp$state[is.na(pmatch(ts_mintemp$state,locations))]

ts_mintemp <- ts_mintemp[-c(which(ts_mintemp$state == "alaska"),
                            which(ts_mintemp$state == "delaware"),
                            which(ts_mintemp$state == "nevada"),
                            which(ts_mintemp$state == "new hampshire"),
                            which(ts_mintemp$state == "rhode island"),
                            which(ts_mintemp$state == "other states")),]

#res <- locations[is.na(pmatch(locations,ts_mintemp$state))]
```

```{r}
# remove means from the time series
locations = unlist(ts_mintemp[,1])
ts = t(ts_mintemp[,-1])
p = length(locations)
medie = colMeans(ts)
ts_no_mean = ts
for (i in 1:p) {
  ts_no_mean[,i] = ts[,i] - medie[i]
}

```
Look at some random Acf:
```{r}
n_plot = 3
plots= vector("list",n_plot*n_plot)
random_indexes = sample(1:p, n_plot*n_plot)
j = 1
for (i in random_indexes) {
  plots[[j]] = ggAcf(ts(ts_no_mean[,i]), lag.max = 29) + labs(title = locations[i])
  j = j+1
}
plot_ACF = do.call(grid.arrange, c(plots, ncol = n_plot, nrow = n_plot))
#install.packages("svglite")
#ggsave("output_plot/ts_acf_random.svg", plot_ACF, width = 10, height = 10)
```
```{r}
# look at some random Pacf
n_plot = 3
plots= vector("list",n_plot*n_plot)
j = 1
for (i in random_indexes) {
  plots[[j]] = ggPacf(ts(ts_no_mean[,i]), lag.max = 29) + labs(title = locations[i])
  j = j+1
}
do.call(grid.arrange, c(plots, ncol = n_plot, nrow = n_plot))
```
An AR(1) in this case is a good approximation of our data.
The ts is clearly annual-stagional, so we must remove stagionality first:
```{r}
matplot(ts_no_mean, type="l")
ts_no_mean_no_seas <- diff(ts_no_mean, differences=1, lag=4)
matplot(ts_no_mean_no_seas, type="l")
```

Let's compare with the best ARIMA:
```{r}
fitted_models_AR1 = vector(mode="list",length = p)
fitted_models_best = vector(mode="list",length = p)
coefficienti_AR1 = rep(0,p)
for(i in 1:p)
{
  fitted_models_AR1[[i]] = arima(ts_no_mean_no_seas[,i], order = c(1,0,0), include.mean = FALSE)
  coefficienti_AR1[i] = fitted_models_AR1[[i]]$coef
  fitted_models_best[[i]] = auto.arima(ts_no_mean_no_seas[,i])
}

aic_AR1 = rep(0,p)
aic_best = rep(0,p)
for(i in 1:p)
{
  aic_AR1[i] = fitted_models_AR1[[i]]$aic
  aic_best[i] = fitted_models_best[[i]]$aic
}
plot_aic = plot(aic_AR1, pch=19, xlab = "Locations", ylab="aic",type='l', main="AIC comparison between AR(1) and the best ARIMA model (no seasonality)")
points(aic_best, pch=19, col="red",type='l')
legend("topleft", legend = c("AR(1)", "Best ARIMA model"), fill = c("black","red"), bty="n", cex=0.8)

```
It seems a reasonable choice!.
Let's do a frequentist estimate of the parameter we want to pass to the algorithm.
```{r}
  p = ncol(ts_no_mean_no_seas)
  fitted_models_AR1 = vector(mode="list",length = p)
  coefficienti_AR = rep(0,p)
  sigma_2_i = rep(0,p)
  
  for(i in 1:p)
  {
    fitted_models_AR1[[i]] = arima(ts_no_mean_no_seas[,i], order = c(1,0,0), include.mean = FALSE)
    coefficienti_AR[i] = fitted_models_AR1[[i]]$coef
    sigma_2_i[i] = fitted_models_AR1[[i]]$sigma2
  }
  
  # stimo i parametri delle prior
  ### rho_h ~ N(rho_0, sigma_2_h / lambda )
  rho_0 = mean(coefficienti_AR)
  sigma_2_avg = mean(sigma_2_i)
  var_rho_h = var(coefficienti_AR)
  lambda = (sigma_2_avg/var_rho_h)
  alpha = (sigma_2_avg)^2 / var(sigma_2_i) + 2
  beta = (alpha-1)*sigma_2_avg
  
  parametri = list(rho_0 = rho_0,
                   lambda = lambda,
                   alpha = alpha,
                   beta = beta)
```
Let's save the output and datasets
```{r}
folder="data_output/min_temp/"
write.table(t(ts_no_mean_no_seas),sep=",", row.names = FALSE, col.names=FALSE , file = paste(folder, "ts.csv", sep=""))
write.table(t(ts),sep=",", row.names = FALSE, col.names=FALSE , file = paste(folder, "ts_mean.csv", sep=""))
write.table(coord[,-3],sep=",", row.names = FALSE, col.names=FALSE , file = paste(folder, "coord.csv", sep=""))
```

From the literature we can see that the approximate radius of the various climatic zones in USA is  1900 km (this is the approximate distance between Oregon and North Dakota). 
Now, we can apply the library bayesmix to cluster our data.

Import the output of bayesmix:
```{r}
source("utils_e/plot_clustering.R")

num_clust = read.csv("data_output/min_temp/numclust.csv", header = FALSE)
names(num_clust)="n"

partitions = read.csv("data_output/min_temp/clustering.csv", header = FALSE)

M <- 0.5
a <- 1900
label <- F
plot_clustering(partitions, coord[,-3], "VI" ,M, a, t(ts_no_mean), label = label)
ob6$plot
ob6$matplot

```
The cluster is not good, indeed if we use the hard cluster parameter bigger than 1900, we will notice that the resulting cluster is 1/2.
That's because the (detrended) temperature time-series behave similarly in each state.
Let's try to cluster precipitatons.



# Precipitation CLUSTER
```{r}

data_prec <- ambient[,c(1,9,6,8)]


ts_prec <- data_prec %>% tidyr::pivot_wider(
  names_from = c("year", "mq"), 
  values_from = Precipitation,
  values_fill = NULL
)

ts_prec$state <- tolower(ts_prec$state)

#check which location is in ts_ that is not in location
#res <- ts_mintemp$state[is.na(pmatch(ts_mintemp$state,locations))]

ts_prec <- ts_prec[-c(which(ts_prec$state == "alaska"),
                            which(ts_prec$state == "delaware"),
                            which(ts_prec$state == "nevada"),
                            which(ts_prec$state == "new hampshire"),
                            which(ts_prec$state == "rhode island"),
                            which(ts_prec$state == "other states")),]

#res <- locations[is.na(pmatch(locations,ts_mintemp$state))]
```

```{r}
# remove means from the time series
locations = unlist(ts_prec[,1])
ts = t(ts_prec[,-1])
p = length(locations)
medie = colMeans(ts)
ts_no_mean = ts
for (i in 1:p) {
  ts_no_mean[,i] = ts[,i] - medie[i]
}
matplot(ts_no_mean, type="l")

```
Look at some random Acf:
```{r}
n_plot = 3
plots= vector("list",n_plot*n_plot)
random_indexes = sample(1:p, n_plot*n_plot)
j = 1
for (i in random_indexes) {
  plots[[j]] = ggAcf(ts(ts_no_mean[,i]), lag.max = 29) + labs(title = locations[i])
  j = j+1
}
plot_ACF = do.call(grid.arrange, c(plots, ncol = n_plot, nrow = n_plot))
#install.packages("svglite")
#ggsave("output_plot/ts_acf_random.svg", plot_ACF, width = 10, height = 10)
```
```{r}
# look at some random Pacf
n_plot = 3
plots= vector("list",n_plot*n_plot)
j = 1
for (i in random_indexes) {
  plots[[j]] = ggPacf(ts(ts_no_mean[,i]), lag.max = 29) + labs(title = locations[i])
  j = j+1
}
do.call(grid.arrange, c(plots, ncol = n_plot, nrow = n_plot))
```
An AR(1) in this case is a good approximation of our data.
Let's compare with the best ARIMA:
```{r}
fitted_models_AR1 = vector(mode="list",length = p)
fitted_models_best = vector(mode="list",length = p)
coefficienti_AR1 = rep(0,p)
for(i in 1:p)
{
  fitted_models_AR1[[i]] = arima(ts_no_mean[,i], order = c(1,0,0), include.mean = FALSE)
  coefficienti_AR1[i] = fitted_models_AR1[[i]]$coef
  fitted_models_best[[i]] = auto.arima(ts_no_mean[,i])
}

aic_AR1 = rep(0,p)
aic_best = rep(0,p)
for(i in 1:p)
{
  aic_AR1[i] = fitted_models_AR1[[i]]$aic
  aic_best[i] = fitted_models_best[[i]]$aic
}
plot_aic = plot(aic_AR1, pch=19, xlab = "Locations", ylab="aic",type='l', main="AIC comparison between AR(1) and the best ARIMA model")
points(aic_best, pch=19, col="red",type='l')
legend("topleft", legend = c("AR(1)", "Best ARIMA model"), fill = c("black","red"), bty="n", cex=0.8)

```
It seems a reasonable chooice despite some peaks down at some points.
Let's do a frequentist estimate of the parameter we want to pass to the algorithm.
```{r}
  p = ncol(ts_no_mean)
  fitted_models_AR1 = vector(mode="list",length = p)
  coefficienti_AR = rep(0,p)
  sigma_2_i = rep(0,p)
  
  for(i in 1:p)
  {
    fitted_models_AR1[[i]] = arima(ts_no_mean[,i], order = c(1,0,0), include.mean = FALSE)
    coefficienti_AR[i] = fitted_models_AR1[[i]]$coef
    sigma_2_i[i] = fitted_models_AR1[[i]]$sigma2
  }
  
  # stimo i parametri delle prior
  ### rho_h ~ N(rho_0, sigma_2_h / lambda )
  rho_0 = mean(coefficienti_AR)
  sigma_2_avg = mean(sigma_2_i)
  var_rho_h = var(coefficienti_AR)
  lambda = (sigma_2_avg/var_rho_h)
  alpha = (sigma_2_avg)^2 / var(sigma_2_i) + 2
  beta = (alpha-1)*sigma_2_avg
  
  parametri = list(rho_0 = rho_0,
                   lambda = lambda,
                   alpha = alpha,
                   beta = beta)
  parametri
```
```{r}
folder="data_output/prec/"
write.table(t(ts_no_mean),sep=",", row.names = FALSE, col.names=FALSE , file = paste(folder, "ts.csv", sep=""))
write.table(t(ts),sep=",", row.names = FALSE, col.names=FALSE , file = paste(folder, "ts_mean.csv", sep=""))
write.table(coord[,-3],sep=",", row.names = FALSE, col.names=FALSE , file = paste(folder, "coord.csv", sep=""))
```

From the literature we can see that the approximate radius of the various climatic zones in USA is  1900 km (this is the approximate distance between Oregon and North Dakota). 
Now, we can apply the library bayesmix to cluster our data.

Import the output of bayesmix:
```{r}
source("utils_e/plot_clustering.R")

num_clust = read.csv("data_output/prec/numclust.csv", header = FALSE)
names(num_clust)="n"

partitions = read.csv("data_output/prec/clustering.csv", header = FALSE)

M <- 0.7
a <- 1900
label <- F
plot_clustering(partitions, coord[,-3], "VI" ,M, a, t(ts_no_mean), label = label)
ob6$plot
ob6$matplot
matplot(ts_no_mean, type="l")
```







